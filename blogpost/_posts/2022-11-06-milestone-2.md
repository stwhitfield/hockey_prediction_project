---
layout: post
title: Milestone 2
---

# 2. Feature Engineering

## Question 2.1

We can look at shots in a number of different ways:

If we look at shot counts by distance, and separate goals and no-goals (Figure 2.1.1) we see that the vast majority of shots are taken within ~60 ft of the goal. Although shots taken are somewhat uniformly distributed in this region (with a clear peak close to the goal), goals are clearly log-distributed, peaking close to the goal and sloping sharply away afterwards. Both shots and goals have a long tail with slight peaking around ~ 170 feet, which may either indicate a particular kind of event or artifacts in the data.

<img src="/pngs/2-2-1a_dist_from_net.png" alt=""> 
<em>Figure 2.1.1. Histogram of shot counts, binned by distance from the net. Goals and no-goals are separated.</em>

If we look at shot counts by angle from the net (basically, from the goalie perspective) (Figure 2.1.2), we see a largely normal distribution centered around angle 0. There is a sharp peak at 0, suggesting that many goals are taken straight-on (this may reflect a certain proportion of shots belonging to shoot-outs). The local peaks around +/- 40 degrees in the distribution of shots taken suggest that players favour shooting from these areas, but the lack of corresponding peaks in the goals distribution suggests that these shots are not particularly successful. 

<img src="/pngs/2-2-1b_angle_from_net.png" alt="">
<em>Figure 2.1.2 Histogram of shot counts, binned by angle from the net. Goals and no-goals are separated.</em>

If we combine the two figures above, we can get a figure relating the distance and angle from net together (Figure 2.1.3). This essentially gives a "bird's eye view" of where shots are taken from, if you place the goalie at (0,0) you can see the spread of angles where shots were taken for all distances.

<img src="/pngs/2-2-1c_2D_hist_jointplot.png" alt="">
<em>Figure 2.1.3 Relationship between shot distance and angle.</em>


## Question 2.2

When we plot the goal rate related to distance from the net (Figure 2.2.1), we see that shots very close to the net are more successful than any others. Although the histogram largely looks like the goals histogram from Figure 2.1.1, the long-distance (~ \> 110 ft) shots look to be disproportionately more successful, with a larger peak on this figure than on Figure 2.1.1.

<img src="/pngs/2-2-2a_goal_rate_dist.png" alt="">
<em>Figure 2.2.1 Goal rate related to distance from the net.</em>

When we plot the goal rate related to angle from the net (Figure 2.2.2), we see a slight peak around angle 0, but the most striking increase is in the shots that are > +/- 90 degrees from the net. This again makes us wonder about artifacts in the data since it seems unlikely that so many shots from behind the net would be so disproportionately successful.

<img src="/pngs/2-2-2b_goal_rate_angle.png" alt="">
<em>Figure 2.2.2 Goal rate related to angle of the shot from the net.</em>

## Question 2.3

When we plot goals binned by distance and separated by empty net and non empty net events (Figure 2.3.1), we see that the vast majority of goals occur when the net is not empty, as we would expect. There are again two main distributions of non empty net goals, between 0 and 60 ft and 150 - 180 ft.

<img src="/pngs/2-2-3a_goals_emptyNet.png" alt="">
<em>Figure 2.3.1 Goals binned by distance. 0 indicates a net that is not empty, and 1 indicates that the net is empty.</em>

 It seems very unlikely that we would see so many non empty net goals at the region 120-180 ft, in the defensive zone of the team taking the shot. Indeed, if we filter the data for events within the defensive zone (ie. > 125 ft from the goal) we find events such as the goal by Ilya Kovalchuk on Jacob Markstrom during the 2018-19 season (gameID 2018020886), which is during a shootout and upon inspection (watching the video at https://www.nhl.com/video/recap-van-4-lak-3-fso/t-277350912/c-65904003) was clearly not from the other side of the ice. This means that we should consider handling shootouts explicitly since there isn't the normal team side indicator that we used to calculate the distance for the other shots. Most of the other shots in this "implausible zone" are from 2015, 2016 or 2017 seasons and, bizarrely, the videos for those seasons don't play properly, making video verification dificult.


# 3. Baseline Models

## Question 3.1

Using a default LogisticRegression classifier trained on distance from net, we obtain 90.2 % accuracy on predicting whether a shot will be a goal. Looking at where the model made mistakes (wrong predictions), it was all in cases where a shot that should have been a goal (1) was predicted as not being a goal (0). This is likely because of the uneven distribution of the two classes (0 and 1). In the training data, fewer than 10 % of shots are goals (9.35 %). Because of this, the model can get more than 90 % accuracy by always predicting that a shot will not be a goal.

## Question 3.2
N/A

## Question 3.3

Three logistic regression classifiers were trained, using distance, angle, or both distance and angle. The results are presented in the following four figures.

<strong>ROC</strong> curves plot true positive (TP) and false positive (FP) rates at different classification thresholds. The better a model (the higher separation it can find between two distributions) the closer the ROC curve gets to 1 on the y axis (the true positive scale). <strong>AUC</strong> (Area Under the ROC Curve) gives a measure of performance across all possible classification thresholds. A model whose predictions are 100% correct has an AUC of 1.0. 

Judging by Figure 3.3.1, models trained using distance from net (distance from net alone or both distance and angle) performed better than those trained using angle from net alone. Models trained on distance tend more towards the top left corner on the ROC plot than do models using angle only, and they have an AUC of 0.65 compared to an AUC of 0.51 for angle. Combining distance and angle adds no increased AUC or curve, suggesting that distance is the defining feature. Indeed, the model using angle only has an AUC matching that of the random baseline. The fact that the ROC curves for angle and random baseline are different despite them having the same AUC values highlights the need to look at data in different ways - in this case, it suggests that the model trained on angle is not simply choosing at random.

<img src="/pngs/2-3-3a_ROC_curves.png" alt="">
<em>Figure 3.3.1 ROC curves for logistic regression models trained on distance, angle, or distance and angle, compared to a random baseline.</em>

Plotting goal rate as a function of shot probability model percentile gives an indication of whether the model favours shots of a certain probability and hints at what the model thinks is a high-quality shot. The random baseline (Figure 3.3.2), as a straight horizonal line at 10% on the goal rate, gives equal weight to goals of all probabilities. The logistic regression model trained on angle gives a higher goal rate to shots around the 50th model percentile and underweights (assigns a lower goal rate to) shots with high or low model percentiles. Similar to the ROC curves, we see that models using distance from net alone and distance + angle have near-identical curves. Both of these models assign a higher-than-baseline goal rate to shots in the upper model percentile (~ \> 70 %) and lower-than-baseline to shots in the lower model percentile (except for shots \< 5 %), suggesting that they attribute a higher quality to shots in the upper model percentile.

<img src="/pngs/2-3-3b_goal_rates.png" alt="">
<em>Figure 3.3.2 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for logistic regression models trained on distance, angle, or distance and angle, compared to a random baseline.</em>

Plotting cumulative proportion of goals as a function of the shot probability model percentile gives an similar story to plotting goal rate. The model trained on angle alone systematically gives less weight to goals in the upper model percentile (\> 50), since by 50 the curve is taking up less than half of the area. This model also gives more weight to goals in the lower 50%. Models trained using distance give a higher weight to shots in the upper model percentile, with an initial steep slope (50% of goals are found in the upper 30%). The sharp increase in proportion at very low shot probability model percentiles for these models again points to some emphasis on very rare shots that may be due to artifacts in our data discussed in question 2.3.

<img src="/pngs/2-3-3c_goal_proportions.png" alt="">
<em>Figure 3.3.3 Cumulative proportion of goals as a function of the shot probability model percentile for logistic regression models trained on distance, angle, or distance and angle, compared to a random baseline.</em>

Calibration curves give an indication of how well a model's predicted probability of event matches the true outcome of that event. A well-calibrated model has a calibration curve that is close to the straight line y=x. All of our logistic regression plots are reasonably close to the straight line, suggesting that they are somewhat well calibrated. All three models clump into the bottom-left corner of the plot, with a fraction of positives less than 0.2 (y-axis). This likely reflects the problem discussed in Question 3.1: none of the models assign a very high probability to goals, probably because goals are much less common than shots.

<img src="/pngs/2-3-3d_calibration_plots.png" alt="">
<em>Figure 3.3.4 Calibration curves for logistic regression models trained on distance, angle, or distance and angle, compared to a random baseline.</em>

The three models were registered on comet.ml as the following: 

Logistic regression trained only on distance: https://www.comet.com/ift6758-project/model-registry/log-reg-basemodel-distance 

Logistic regression trained only on angle: https://www.comet.com/ift6758-project/model-registry/log-reg-basemodel-angle 

Logistic regression trained on distance and angle: 
https://www.comet.com/ift6758-project/model-registry/log-reg-basemodel-distance-angle

Experiments associated with these models are found under the Source Experiment header of the registered models or at:

| log_reg_basemodel_distance | https://www.comet.com/ift6758-project/ift6758-project/1ee938cedd5f494f960b72cc332ce50d |
| log_reg_basemodel_angle | https://www.comet.com/ift6758-project/ift6758-project/53cb3c31fe9c423093e87ab5ee01cada |
| log_reg_basemodel_distance_angle | https://www.comet.com/ift6758-project/ift6758-project/ab71fd28586f46249a226b387912fd87 |


# 4. Feature Engineering II

## Question 4.1

N/A

## Question 4.2
N/A

## Question 4.3
N/A

## (Bonus) Question 4.4

N/A

## Question 4.5

Features in the dataframe generated in this section are the following:

<strong>gameId</strong>: 10-digit identifier encoding information about when the game occurred and what type of game it was.  
<strong>time</strong>: Total time elapsed in the game, given as number of seconds since start (00:00).  
<strong>period</strong>: Game period.  
<strong>coordinateX</strong>: X-coordinate on the rink of the event (centre of the rink is (0,0).  
<strong>coordinateY</strong>: Y-coordinate on the rink of the event (centre of the rink is (0,0).  
<strong>shotDistance</strong>: Distance in feet from the target net to the point that the shot took place.  
<strong>shotAngle</strong>: Angle in degrees between the net and the point that the shot took place, from the perspective of the goalie (looking straight ahead is 0 degrees).  
<strong>shotType</strong>: Type of the shot (e.g. Snap Shot, Backhand).  
<strong>emptyNet</strong>: Whether the shot was made against an empty net (1) or not (0).  
<strong>lastEventType</strong>: Type of the shot (e.g. Snap Shot, Backhand) of the event immediately preceding this shot.  
<strong>lastCoordinateX</strong>: X-coordinate on the rink of the event immediately preceding this shot.   
<strong>lastCoordinateY</strong>: Y-coordinate on the rink of the event immediately preceding this shot.  
<strong>lastTime</strong>: Time since the event immediately preceding this shot, given as number of seconds.  
<strong>lastShotDistance</strong>: Distance in feet from the location of the event immediately preceding this shot to the location of this shot.  
<strong>rebound</strong>: Whether or not the event immediately preceding this shot was also a shot.  
<strong>changeShotAngle</strong>: If the event immediately preceding this shot was also a shot, gives the change in angle between the two.  
<strong>speed</strong>: Distance from previous shot to this shot, divided by the time since the previous shot.  
<strong>friendlyPlayersOnIce</strong>: Number of friendly non-goalie skaters (ie. teammates) on the ice.  
<strong>opposingPlayersOnIce</strong>: Number of opposing non-goalie skaters on the ice.  
<strong>timeSincePP</strong>: Time since the power-play started, in seconds.  
<strong>isGoal</strong>: Whether or not the event resulted in a goal.  

We filtered our dataset to the Winnipeg vs Washington game on March 12, 2018. The experiment that stores the filtered DataFrame is 
https://www.comet.com/ift6758-project/feature-engineering-data/ and the file is under Assets and Artifacts/dataframes as "wpg_v_wsh_2017021065.csv".


# 5. Advanced Models

## Question 5.1

We performed an 80:20 train:validation split of our training data before training an XGBClassifier() with default settings, which corresponds to a tree classifier with depth 6. The results of the classifier are given in Figures 5.1.1-4.

<img src="/pngs/2-5-1a_ROC_curves.png" alt="">
<em>Figure 5.1.1 ROC curves for XGBoost classifiers trained on distance, angle, or distance and angle, compared to a random baseline.</em>

<img src="/pngs/2-5-1b_goal_rates.png" alt="">
<em>Figure 5.1.2 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for XGBoost classifiers trained on distance, angle, or distance and angle, compared to a random baseline.</em>

<img src="/pngs/2-5-1c_goal_proportions.jpeg" alt="">
<em>Figure 5.1.3 Cumulative proportion of goals as a function of the shot probability model percentile for XGBoost classifiers trained on distance, angle, or distance and angle, compared to a random baseline.</em>

<img src="/pngs/2-5-1d_calibration_plots.png" alt="">
<em>Figure 5.1.4 Calibration curves for XGBoost classifiers trained on distance, angle, or distance and angle, compared to a random baseline.</em>

Generally, default XGBoost classifiers seem to do better than logistic regression models. The lowest AUC for XGBoost (0.65, with angle only) is comparable to the highest AUC for logistic regression (0.65, with distance or distance + angle) and adding distance makes XGBoost do considerably better. XGBoost gets a slight increase by combining distance and angle features (AUC 0.73, up from AUC 0.71), whereas logistic regression classifiers did not see any improvement when using the two features together. The goal rate graphs and cumulative goal percentages for XGBoost show similar trends to the better logistic regression models. XGBoost also gives a broader range of predicted probabilities than logistic regression models, suggesting that it is better able to handle the fact that goals are relatively rare. Although XGBoos is well calibrated for lower-probability events it varies greatly for events with a higher mean predicted probability.

The relevant comet.ml entries for this experiment can be found at the following:

| xgb_distance | https://www.comet.com/ift6758-project/ift6758-project/6ed9abab4689491a85d8029f14bd5b40 |
| xgb_angle | https://www.comet.com/ift6758-project/ift6758-project/eb4cb50d1450438fbc8b2c94de60ec43 |
| xgb_distance_angle | https://www.comet.com/ift6758-project/ift6758-project/6019b22bd3d54c64881c6c572898e492 |

## Question 5.2

To search the feature space more efficiently than a grid search, we used a RandomizedSearchCV() (randomized search on hyper parameters, optimized by cross-validation) that tries out a fixed number of parameter settings sampled from specified distributions. After training a default XGBoost classifier on all the features created in Part 4 (Figures 5.2.1), we tested the following range of hyper-parameters:

| n_estimators | max_depth | learning_rate| booster | gamma | reg_alpha | reg_lambda |
|--------------|-----------|--------------|---------|-------|-----------|------------|
| 100 | 3  | 0.01 | 'gbtree' | 0| 0 | 0.5 |
| 200 | 6  | 0.05 | 'gblinear' | 0.5| 0.5 | 1 |
| 500 | 10 | 0.1 | 'dart' | 1| 1 | 5 |

The best performing model from the selected hyperparameters was: 

| n_estimators | max_depth | learning_rate| booster | gamma | reg_alpha | reg_lambda |
|--------------|-----------|--------------|---------|-------|-----------|------------|
| 200 | 10  | 0.05 | 'dart' | 0.5 | 0 | 5 |

Curves corresponding to this optimized model are given in Figures 5.2.2-5:

<img src="/pngs/2-5-2a_xgb_all_features_default_hypers_roc_curve.png" alt="">
<em>Figure 5.2.1 ROC curve for a default XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

<img src="/pngs/2-5-2a_roc_curve.png" alt="">
<em>Figure 5.2.2 ROC curve for a randomized search with cross-validation optimized XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

<img src="/pngs/2-5-2b_goal_rate_plot.png" alt="">
<em>Figure 5.2.3 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for a randomized search with cross-validation optimized XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

<img src="/pngs/2-5-2c_cumulative_goal_rate.png" alt="">
<em>Figure 5.2.4 Cumulative proportion of goals as a function of the shot probability model percentile for a randomized search with cross-validation optimized XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

<img src="/pngs/2-5-2d_calibration_curve.png" alt=""> 
<em>Figure 5.2.5 Calibration curve for a randomized search with cross-validation optimized XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

The hyper-parameter-optimized XGBoost model trained on all features performed slightly better than the default XGBoost baseline trained on all features, with an AUC of 0.81 (compared to the AUC of 0.80) (Figure 5.2.2). Both performed significantly better than XGBoost models from question 5.1, which had a top AUC of 0.71. The optimized model with all features had increased discrimination in the goal rate graph, with a sharp inflection point around 90% shot probability model percentile. The model was also significantly better calibrated, matching the "perfectly calibrated" line very well until around 0.55 mean predicted probability but not departing too much from calibration even after that point.

This model is registered as xgb-all-features-grid-search-best-model at https://www.comet.com/ift6758-project/model-registry/xgb-all-features-grid-search-best-model and the experiment can be found there as xgb_all_features_grid_search under the Source Experiments header or at https://www.comet.com/ift6758-project/ift6758-project/78ed818f94a84ea0be6c008bb66235ad .


## Question 5.3

Feature selection strategies that we used to try to simplify our input features are summarized with some key metrics in the following table:

|strategy| accuracy after fs | f1_score | ROC AUC | # features after selection
| Lasso| 0.907 | 0.179 | 0.802 | 16 |
| Recursive feature elimination (RFE) | 0.908 | 0.185 | 0.806 | 15 |
| Univariate | 0.907 | 0.172 | 0.801 | 15 |
| Variance threshold (0.25) | 0.904 | 0.146 | 0.795 | 17 |

F1 score is a measure of a test's accuracy that takes into account both precision and recall. An F1 score of 1 is best and 0 is worst.

The results above suggest that none of the feature selection strategies we attempted significantly decreased our feature space using the conditions we tried. We proceeded with RFE for model evaluation (Figures 5.3.1-4) based on computational speed and that it obtained slightly higher scores than the other techniques. RFE found that coordinateX, lastCoordinateX, speed, and emptyNet were the most dispensable variables.

<img src="/pngs/2-5-3a_roc_curve.png" alt="">
<em>Figure 5.3.1 ROC curve for a XGBoost classifier trained using hyperparameters selected in 5.2 on a subset of features created in task 4.</em>

<img src="/pngs/2-5-3b_goal_rate_plot.png" alt="">
<em>Figure 5.3.2 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for a XGBoost classifier trained using hyperparameters selected in 5.2 on a subset of features created in task 4.</em>

<img src="/pngs/2-5-3c_cumulative_goal_rate.png" alt="">
<em>Figure 5.3.3 Cumulative proportion of goals as a function of the shot probability model percentile for a XGBoost classifier trained using hyperparameters selected in 5.2 on a subset of features created in task 4.</em>

<img src="/pngs/2-5-3d_calibration_curve.png" alt=""> 
<em>Figure 5.3.4 Calibration curve for a XGBoost classifier trained using hyperparameters selected in 5.2 on a subset of features created in task 4.</em>

Results with this slightly-decreased number of features virtually mirror those obtained in 5.2, which is not surprising since most of the features are the same. They do suggest that the features we dropped (coordinateX, lastCoordinateX, speed, and emptyNet) are indeed redundant with other features in the dataset.

This model is registered as xgb-featsel-rfe-best-model-params at https://www.comet.com/ift6758-project/model-registry/xgb-featsel-rfe-best-model-params and the experiment can be found there as xgb_all_features_grid_search under the Source Experiments header or at https://www.comet.com/ift6758-project/ift6758-project/9b2c025b46ae48f6a346d4142ce207e9.


# 6. Give it your best shot!

Now it's your turn to come up with the best model you can for predicting expected goals! For this section you are free to try any sort of models that you’d like. Here are some potential things you can try (you’re encouraged to try your own ideas!): 
Different model types such as neural networks, decision trees, clustering, etc.
More advanced feature selection strategies
More thoughtful splitting of the training data 
For example, motivated by the fact that teams often start completely healthy but then lose players to injury as the season progresses, you could omit the first 20 games per team, or select the training/validation sets by alternating games.
Hyperparameter tuning, cross validation strategies
Regularization
Explore more novel metrics for this task
Don’t spend too much time trying to make this part perfect. The goal here is to try a wide variety of things rather than trying to hyper-optimize a single approach. To get full marks, we expect to see at least 3-4 different things attempted.

## Question 6.1

In your blog post, discuss the various techniques and methods you tried. Include the same four figures as in Part 3 (ROC/AUC curve, goal rate vs probability percentile, cumulative proportion of goals vs probability percentile, and the reliability curve). Quantitative metrics are only required for a few set of experiments, so you only need to include a few curves on each plot (eg. things that you found interesting, or models that performed particularly well). Make sure to include and highlight what you consider your best ‘final’ model. For methods that weren’t too successful or interesting, you can just include them as short qualitative discussion.

## Question 6.2

Next to the figures, include links to the experiment entry in your comet.ml projects that you included quantitative metrics for (around 3-4). Log the models to the experiments on comet.ml (example here) and register them with some informative tags. 


# 7. Evaluate on test set

## Question 7.1

Test your 5 models on the untouched 2019/20 regular season dataset. In your blogpost, include the four figures described above. Discuss your results and observations on the test set. Do your models perform as well on the test set as you did on your validation set when building your models? Do any models perform better or worse than you expected?

## Question 7.2

Test your 5 models on the untouched 2019/20 playoff games. In your blogpost, include the four figures described above. Discuss your results and observations on this test set. Are there any differences to the regular season test set or do you get similar ‘generalization’ performance?
