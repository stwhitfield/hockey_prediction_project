---
layout: post
title: Milestone 2
---

# 2. Feature Engineering

## Question 2.1

We can look at shots in a number of different ways:

If we look at shot counts by distance, and separate goals and no-goals (Figure 2.1.1) we see that the vast majority of shots are taken within ~60 ft of the goal. Although shots taken are somewhat uniformly distributed in this region (with a clear peak close to the goal), goals are clearly log-distributed, peaking close to the goal and sloping sharply away afterwards. Both shots and goals have a long tail with slight peaking around ~ 170 feet, which may either indicate a particular kind of event or artifacts in the data.

<img src="/pngs/2-2-1a_dist_from_net.png" alt=""> 
<em>Figure 2.1.1. Histogram of shot counts, binned by distance from the net. Goals and no-goals are separated.</em>

If we look at shot counts by angle from the net (basically, from the goalie perspective) (Figure 2.1.2), we see a largely normal distribution centered around angle 0. There is a sharp peak at 0, suggesting that many goals are taken straight-on (this may reflect a certain proportion of shots belonging to shoot-outs). The local peaks around +/- 40 degrees in the distribution of shots taken suggest that players favour shooting from these areas, but the lack of corresponding peaks in the goals distribution suggests that these shots are not particularly successful. 

<img src="/pngs/2-2-1b_angle_from_net.png" alt="">
<em>Figure 2.1.2 Histogram of shot counts, binned by angle from the net. Goals and no-goals are separated.</em>

If we combine the two figures above, we can get a figure relating the distance and angle from net together (Figure 2.1.3). This essentially gives a "bird's eye view" of where shots are taken from, if you place the goalie at (0,0) you can see the spread of angles where shots were taken for all distances.

<img src="/pngs/2-2-1c_2D_hist_jointplot.png" alt="">
<em>Figure 2.1.3 Relationship between shot distance and angle.</em>


## Question 2.2

When we plot the goal rate related to distance from the net (Figure 2.2.1), we see that shots very close to the net are more successful than any others. Although the histogram largely looks like the goals histogram from Figure 2.1.1, the long-distance (~ \> 110 ft) shots look to be disproportionately more successful, with a larger peak on this figure than on Figure 2.1.1.

<img src="/pngs/2-2-2a_goal_rate_dist.png" alt="">
<em>Figure 2.2.1 Goal rate related to distance from the net.</em>

When we plot the goal rate related to angle from the net (Figure 2.2.2), we see a slight peak around angle 0, but the most striking increase is in the shots that are > +/- 90 degrees from the net. This again makes us wonder about artifacts in the data since it seems unlikely that so many shots from behind the net would be so disproportionately successful.

<img src="/pngs/2-2-2b_goal_rate_angle.png" alt="">
<em>Figure 2.2.2 Goal rate related to angle of the shot from the net.</em>

## Question 2.3

When we plot goals binned by distance and separated by empty net and non empty net events (Figure 2.3.1), we see that the vast majority of goals occur when the net is not empty, as we would expect. There are again two main distributions of non empty net goals, between 0 and 60 ft and 150 - 180 ft.

<img src="/pngs/2-2-3a_goals_emptyNet.png" alt="">
<em>Figure 2.3.1 Goals binned by distance. 0 indicates a net that is not empty, and 1 indicates that the net is empty.</em>

 It seems very unlikely that we would see so many non empty net goals at the region 120-180 ft, in the defensive zone of the team taking the shot. Indeed, if we filter the data for events within the defensive zone (ie. > 125 ft from the goal) we find events such as the goal by Ilya Kovalchuk on Jacob Markstrom during the 2018-19 season (gameID 2018020886), which is during a shootout and upon inspection (watching the video at https://www.nhl.com/video/recap-van-4-lak-3-fso/t-277350912/c-65904003) was clearly not from the other side of the ice. This means that we should consider handling shootouts explicitly since there isn't the normal team side indicator that we used to calculate the distance for the other shots. Most of the other shots in this "implausible zone" are from 2015, 2016 or 2017 seasons and, bizarrely, the videos for those seasons don't play properly, making video verification dificult.


# 3. Baseline Models

## Question 3.1

Using a default LogisticRegression classifier trained on distance from net, we obtain 90.2 % accuracy on predicting whether a shot will be a goal. Looking at where the model made mistakes (wrong predictions), it was all in cases where a shot that should have been a goal (1) was predicted as not being a goal (0). This is likely because of the uneven distribution of the two classes (0 and 1). In the training data, fewer than 10 % of shots are goals (9.35 %). Because of this, the model can get more than 90 % accuracy by always predicting that a shot will not be a goal.

## Question 3.3

Three logistic regression classifiers were trained, using distance, angle, or both distance and angle. The results are presented in the following four figures.

<strong>ROC</strong> curves plot true positive (TP) and false positive (FP) rates at different classification thresholds. The better a model (the higher separation it can find between two distributions) the closer the ROC curve gets to 1 on the y axis (the true positive scale). <strong>AUC</strong> (Area Under the ROC Curve) gives a measure of performance across all possible classification thresholds. A model whose predictions are 100% correct has an AUC of 1.0. 

Judging by Figure 3.3.1, models trained using distance from net (distance from net alone or both distance and angle) performed better than those trained using angle from net alone. Models trained on distance tend more towards the top left corner on the ROC plot than do models using angle only, and they have an AUC of 0.65 compared to an AUC of 0.51 for angle. Combining distance and angle adds no increased AUC or curve, suggesting that distance is the defining feature. Indeed, the model using angle only has an AUC matching that of the random baseline. The fact that the ROC curves for angle and random baseline are different despite them having the same AUC values highlights the need to look at data in different ways - in this case, it suggests that the model trained on angle is not simply choosing at random.

<img src="/pngs/2-3-3a_ROC_curves.png" alt="">
<em>Figure 3.3.1 ROC curves for logistic regression models trained on distance, angle, or distance and angle, compared to a random baseline.</em>

Plotting goal rate as a function of shot probability model percentile gives an indication of whether the model favours shots of a certain probability and hints at what the model thinks is a high-quality shot. The random baseline (Figure 3.3.2), as a straight horizonal line at 10% on the goal rate, gives equal weight to goals of all probabilities. The logistic regression model trained on angle gives a higher goal rate to shots around the 50th model percentile and underweights (assigns a lower goal rate to) shots with high or low model percentiles. Similar to the ROC curves, we see that models using distance from net alone and distance + angle have near-identical curves. Both of these models assign a higher-than-baseline goal rate to shots in the upper model percentile (~ \> 70 %) and lower-than-baseline to shots in the lower model percentile (except for shots \< 5 %), suggesting that they attribute a higher quality to shots in the upper model percentile.

<img src="/pngs/2-3-3b_goal_rates.png" alt="">
<em>Figure 3.3.2 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for logistic regression models trained on distance, angle, or distance and angle, compared to a random baseline.</em>

Plotting cumulative proportion of goals as a function of the shot probability model percentile gives an similar story to plotting goal rate. The model trained on angle alone systematically gives less weight to goals in the upper model percentile (\> 50), since by 50 the curve is taking up less than half of the area. This model also gives more weight to goals in the lower 50%. Models trained using distance give a higher weight to shots in the upper model percentile, with an initial steep slope (50% of goals are found in the upper 30%). The sharp increase in proportion at very low shot probability model percentiles for these models again points to some emphasis on very rare shots that may be due to artifacts in our data discussed in question 2.3.

<img src="/pngs/2-3-3c_goal_proportions.png" alt="">
<em>Figure 3.3.3 Cumulative proportion of goals as a function of the shot probability model percentile for logistic regression models trained on distance, angle, or distance and angle, compared to a random baseline.</em>

Calibration curves give an indication of how well a model's predicted probability of event matches the true outcome of that event. A well-calibrated model has a calibration curve that is close to the straight line y=x. All of our logistic regression plots are reasonably close to the straight line, suggesting that they are somewhat well calibrated. All three models clump into the bottom-left corner of the plot, with a fraction of positives less than 0.2 (y-axis). This likely reflects the problem discussed in Question 3.1: none of the models assign a very high probability to goals, probably because goals are much less common than shots.

<img src="/pngs/2-3-3d_calibration_plots.png" alt="">
<em>Figure 3.3.4 Calibration curves for logistic regression models trained on distance, angle, or distance and angle, compared to a random baseline.</em>

The three models were registered on comet.ml as the following: 

Logistic regression trained only on distance: https://www.comet.com/ift6758-project/model-registry/log-reg-basemodel-distance 

Logistic regression trained only on angle: https://www.comet.com/ift6758-project/model-registry/log-reg-basemodel-angle 

Logistic regression trained on distance and angle: 
https://www.comet.com/ift6758-project/model-registry/log-reg-basemodel-distance-angle

Experiments associated with these models are found under the Source Experiment header of the registered models or at:

| log_reg_basemodel_distance | https://www.comet.com/ift6758-project/ift6758-project/1ee938cedd5f494f960b72cc332ce50d |
| log_reg_basemodel_angle | https://www.comet.com/ift6758-project/ift6758-project/53cb3c31fe9c423093e87ab5ee01cada |
| log_reg_basemodel_distance_angle | https://www.comet.com/ift6758-project/ift6758-project/ab71fd28586f46249a226b387912fd87 |


# 4. Feature Engineering II

## Question 4.5

Features in the dataframe generated in this section are the following:

<strong>gameId</strong>: 10-digit identifier encoding information about when the game occurred and what type of game it was.  
<strong>time</strong>: Total time elapsed in the game, given as number of seconds since start (00:00).  
<strong>period</strong>: Game period.  
<strong>coordinateX</strong>: X-coordinate on the rink of the event (centre of the rink is (0,0).  
<strong>coordinateY</strong>: Y-coordinate on the rink of the event (centre of the rink is (0,0).  
<strong>shotDistance</strong>: Distance in feet from the target net to the point that the shot took place.  
<strong>shotAngle</strong>: Angle in degrees between the net and the point that the shot took place, from the perspective of the goalie (looking straight ahead is 0 degrees).  
<strong>shotType</strong>: Type of the shot (e.g. Snap Shot, Backhand).  
<strong>emptyNet</strong>: Whether the shot was made against an empty net (1) or not (0).  
<strong>lastEventType</strong>: Type of the shot (e.g. Snap Shot, Backhand) of the event immediately preceding this shot.  
<strong>lastCoordinateX</strong>: X-coordinate on the rink of the event immediately preceding this shot.   
<strong>lastCoordinateY</strong>: Y-coordinate on the rink of the event immediately preceding this shot.  
<strong>lastTime</strong>: Time since the event immediately preceding this shot, given as number of seconds.  
<strong>lastShotDistance</strong>: Distance in feet from the location of the event immediately preceding this shot to the location of this shot.  
<strong>rebound</strong>: Whether or not the event immediately preceding this shot was also a shot.  
<strong>changeShotAngle</strong>: If the event immediately preceding this shot was also a shot, gives the change in angle between the two.  
<strong>speed</strong>: Distance from previous shot to this shot, divided by the time since the previous shot.  
<strong>friendlyPlayersOnIce</strong>: Number of friendly non-goalie skaters (ie. teammates) on the ice.  
<strong>opposingPlayersOnIce</strong>: Number of opposing non-goalie skaters on the ice.  
<strong>timeSincePP</strong>: Time since the power-play started, in seconds.  
<strong>isGoal</strong>: Whether or not the event resulted in a goal.  

We filtered our dataset to the Winnipeg vs Washington game on March 12, 2018. The experiment that stores the filtered DataFrame is 
https://www.comet.com/ift6758-project/feature-engineering-data/ and the file is under Assets and Artifacts/dataframes as "wpg_v_wsh_2017021065.csv".


# 5. Advanced Models

## Question 5.1

We performed an 80:20 train:validation split of our training data before training an XGBClassifier() with default settings, which corresponds to a tree classifier with depth 6. The results of the classifier are given in Figures 5.1.1-4.

<img src="/pngs/2-5-1a_ROC_curves.png" alt="">
<em>Figure 5.1.1 ROC curves for XGBoost classifiers trained on distance, angle, or distance and angle, compared to a random baseline.</em>

<img src="/pngs/2-5-1b_goal_rates.png" alt="">
<em>Figure 5.1.2 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for XGBoost classifiers trained on distance, angle, or distance and angle, compared to a random baseline.</em>

<img src="/pngs/2-5-1c_goal_proportions.jpeg" alt="">
<em>Figure 5.1.3 Cumulative proportion of goals as a function of the shot probability model percentile for XGBoost classifiers trained on distance, angle, or distance and angle, compared to a random baseline.</em>

<img src="/pngs/2-5-1d_calibration_plots.png" alt="">
<em>Figure 5.1.4 Calibration curves for XGBoost classifiers trained on distance, angle, or distance and angle, compared to a random baseline.</em>

Generally, default XGBoost classifiers seem to do better than logistic regression models. The lowest AUC for XGBoost (0.65, with angle only) is comparable to the highest AUC for logistic regression (0.65, with distance or distance + angle) and adding distance makes XGBoost do considerably better. XGBoost gets a slight increase by combining distance and angle features (AUC 0.73, up from AUC 0.71), whereas logistic regression classifiers did not see any improvement when using the two features together. The goal rate graphs and cumulative goal percentages for XGBoost show similar trends to the better logistic regression models, with XGBoost discriminating better and applying more value (up to a 30% success rate) to goals in the upper 10 percentile of model shot probability. XGBoost also gives a broader range of predicted probabilities than logistic regression models on the calibration plot, suggesting that it is better able to handle the fact that goals are relatively rare. Although XGBoos is well calibrated for lower-probability events it varies greatly for events with a higher mean predicted probability.

The relevant comet.ml entries for this experiment can be found at the following:

| xgb_distance | https://www.comet.com/ift6758-project/ift6758-project/6ed9abab4689491a85d8029f14bd5b40 |
| xgb_angle | https://www.comet.com/ift6758-project/ift6758-project/eb4cb50d1450438fbc8b2c94de60ec43 |
| xgb_distance_angle | https://www.comet.com/ift6758-project/ift6758-project/6019b22bd3d54c64881c6c572898e492 |

## Question 5.2

To search the feature space more efficiently than a grid search, we used a RandomizedSearchCV() (randomized search on hyper parameters, optimized by cross-validation) that tries out a fixed number of parameter settings sampled from specified distributions. After training a default XGBoost classifier on all the features created in Part 4 (Figures 5.2.1), we tested the following range of hyper-parameters:

| <strong>n_estimators</strong> | <strong>max_depth</strong> | <strong>learning_rate</strong>| <strong>booster</strong> | <strong>gamma</strong> | <strong>reg_alpha</strong> | <strong>reg_lambda</strong> |
|--------------|-----------|--------------|---------|-------|-----------|------------|
| 100 | 3  | 0.01 | 'gbtree' | 0| 0 | 0.5 |
| 200 | 6  | 0.05 | 'gblinear' | 0.5| 0.5 | 1 |
| 500 | 10 | 0.1 | 'dart' | 1| 1 | 5 |

The best performing model from the selected hyperparameters was: 

| <strong>n_estimators</strong> | <strong>max_depth</strong> | <strong>learning_rate</strong>| <strong>booster</strong> | <strong>gamma</strong> | <strong>reg_alpha</strong> | <strong>reg_lambda</strong> |
|--------------|-----------|--------------|---------|-------|-----------|------------|
| 200 | 10  | 0.05 | 'dart' | 0.5 | 0 | 5 |

Curves corresponding to this optimized model are given in Figures 5.2.2-5:

<img src="/pngs/2-5-2a_xgb_all_features_default_hypers_roc_curve.png" alt="">
<em>Figure 5.2.1 ROC curve for a default XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

<img src="/pngs/2-5-2a_roc_curve.png" alt="">
<em>Figure 5.2.2 ROC curve for a randomized search with cross-validation optimized XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

<img src="/pngs/2-5-2b_goal_rate_plot.png" alt="">
<em>Figure 5.2.3 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for a randomized search with cross-validation optimized XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

<img src="/pngs/2-5-2c_cumulative_goal_rate.png" alt="">
<em>Figure 5.2.4 Cumulative proportion of goals as a function of the shot probability model percentile for a randomized search with cross-validation optimized XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

<img src="/pngs/2-5-2d_calibration_curve.png" alt=""> 
<em>Figure 5.2.5 Calibration curve for a randomized search with cross-validation optimized XGBoost classifier trained on all features created in task 4, compared to a random baseline.</em>

The hyper-parameter-optimized XGBoost model trained on all features performed slightly better than the default XGBoost baseline trained on all features, with an AUC of 0.81 (compared to the AUC of 0.80) (Figure 5.2.2). Both performed significantly better than XGBoost models from question 5.1, which had a top AUC of 0.71. The optimized model with all features had increased discrimination in the goal rate graph, with a peak at around 50% goal success (y-axis) and a sharp slope down to around 90% shot probability model percentile. The model was also significantly better calibrated, matching the "perfectly calibrated" line very well until around 0.55 mean predicted probability but not departing too much from calibration even after that point.

This model is registered as xgb-all-features-grid-search-best-model at https://www.comet.com/ift6758-project/model-registry/xgb-all-features-grid-search-best-model and the experiment can be found there as xgb_all_features_grid_search under the Source Experiments header or at https://www.comet.com/ift6758-project/ift6758-project/78ed818f94a84ea0be6c008bb66235ad .


## Question 5.3

Feature selection strategies that we used to try to simplify our input features are summarized with some key metrics in the following table:

|<strong>Strategy</strong>| <strong>Overall Accuracy after Feature Selection</strong> | <strong>F1 Score</strong> | <strong>ROC AUC</strong> | <strong># Features After Selection</strong>
| Lasso| 0.907 | 0.179 | 0.802 | 16 |
| Recursive feature elimination (RFE) | 0.908 | 0.185 | 0.806 | 15 |
| Univariate | 0.907 | 0.172 | 0.801 | 15 |
| Variance threshold (0.25) | 0.904 | 0.146 | 0.795 | 17 |

F1 score is a measure of a test's accuracy that takes into account both precision and recall. An F1 score of 1 is best and 0 is worst. As a comparison to the numbers above, our XGBoost model trained only on distance and angle obtained an F1 score of 0.009.

The results above suggest that none of the feature selection strategies we attempted hugely decreased our feature space using the conditions we tried. We proceeded with RFE for model evaluation (Figures 5.3.1-4) based on computational speed and that it obtained slightly higher scores than the other techniques. RFE found that coordinateX, lastCoordinateX, speed, and emptyNet were the most dispensable variables.

<img src="/pngs/2-5-3a_roc_curve.png" alt="">
<em>Figure 5.3.1 ROC curve for a XGBoost classifier trained using hyperparameters selected in 5.2 on a subset of features created in task 4.</em>

<img src="/pngs/2-5-3b_goal_rate_plot.png" alt="">
<em>Figure 5.3.2 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for a XGBoost classifier trained using hyperparameters selected in 5.2 on a subset of features created in task 4.</em>

<img src="/pngs/2-5-3c_cumulative_goal_rate.png" alt="">
<em>Figure 5.3.3 Cumulative proportion of goals as a function of the shot probability model percentile for a XGBoost classifier trained using hyperparameters selected in 5.2 on a subset of features created in task 4.</em>

<img src="/pngs/2-5-3d_calibration_curve.png" alt=""> 
<em>Figure 5.3.4 Calibration curve for a XGBoost classifier trained using hyperparameters selected in 5.2 on a subset of features created in task 4.</em>

Results with this slightly-decreased number of features virtually mirror those obtained in 5.2, which is not surprising since most of the features are the same. They do suggest that the features we dropped (coordinateX, lastCoordinateX, speed, and emptyNet) are indeed redundant with other features in the dataset. 

This model is registered as xgb-featsel-rfe-best-model-params at https://www.comet.com/ift6758-project/model-registry/xgb-featsel-rfe-best-model-params and the experiment can be found there as xgb_all_features_grid_search under the Source Experiments header or at https://www.comet.com/ift6758-project/ift6758-project/9b2c025b46ae48f6a346d4142ce207e9.


# 6. Give it your best shot!

## Questions 6.1-2

We applied a range of models to this problem, the results of which are summarized in the following table with some key metrics. Model % at cumulative 50% indicates the shot probability model percentile at which cumulatively 50% of shots were found. Goal Rate Inflection indicates the max and min goal success (goals/(shots+goals)) in the upper 10th shot probability model percentile (that is, from 100% to 90%) and thereby gives a sense of how discriminatory the model is towards "quality" shots.

| <strong>Method</strong> | <strong>Overall Accuracy</strong> | <strong>F1 Score</strong> | <strong>ROC AUC</strong> | <strong>Model % at cumulative 50%</strong> | <strong>Goal Rate Inflection</strong> | <strong>Experiment link</strong> |
| Random Forest with grid search | 0.916 | 0.296 | 0.804| 84 | 57-23 | https://www.comet.com/ift6758-project/ift6758-project/64c70835c4fc4752aaadb20c802b29fc |
|Random Forest with RFE feature selection | 0.916 | 0.294 | 0.778 | 82 | 53-21 | |
| Random Forest with PCA dimensionality reduction | 0.912 | 0.231 | 0.767 | 80 | 47-21 | |
| Decision Trees with grid search | 0.905 | 0.126 | 0.772 | 80 | 42-25 | https://www.comet.com/ift6758-project/ift6758-project/0c17d9aa6a594feab44c42230333cbbd |
| Decision Tree with RFE feature selection | 0.905 | 0.126 | 0.752 | 80 | 42-25 | |
| Decision Tree with PCA dimensionality reduction | 0.901 | 0.058 | 0.717 | 76 | 33-21| |
| Gaussian Naive Bayes | 0.891 | 0.210 | 0.693 | 72 | 34-19 | https://www.comet.com/ift6758-project/ift6758-project/e3f0b5b9ebab4a1f940d5a9bd66666cc |
| SVM | 0.900 | 0 | 0.553 | 56 | 20-11| https://www.comet.com/ift6758-project/ift6758-project/ce163758d968487595f517b1ce6467a5 |
| Neural Network | 0.656 | 0.311 | 0.785 | 82 | 43-26 | https://www.comet.com/ift6758-project/ift6758-project/e6b7d3dc713f4c73b3b7292ab81c7acc |

Plotting all of these curves on the same graph would be excessively busy. Individual curves for the models with experiments reported can be viewed at https://www.comet.com/ift6758-project/ift6758-project/view/new/panels. For each model, unless otherwise mentioned, we used randomized grid search with cross-validation to choose hyper-parameters. We also investigated combining different feature selection techniques with each model. Our neural network (Figures 6.1.1-4) was a Keras Sequential model with three dense layers of 2014 neurons and two dropout layers of 1024 neurons, trained for 100 epochs with a batch size of 2048. Although by the metrics presented in the above table our neural network model has the lowest total accuracy of the models we tried, we believe that it is one of the better-performing models due to its high F1 score. As shown in previous questions (most notably 3.1), overall accuracy is a poor metric and the significantly better F1 score shows that the model is being more disciminatory about what it picks as a goal. Overall, the feature selection techniques we tried did not improve the performance of our models, suggesting that more complex feature engineering might be required. Gaussian Naive Bayes and SVM models performed rather poorly compared to other methods, suggesting that these models lack the capacity to appropriately capture the complexity of interactions between these features. 

<img src="/pngs/2-6-1a_best_nn_roc_curve.png" alt="">
<em>Figure 6.1.1 ROC curve for neural network model.</em>

<img src="/pngs/2-6-1b_best_nn_goal_rate_plot.png" alt="">
<em>Figure 6.1.2 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for neural network model.</em>

<img src="/pngs/2-6-1c_best_nn_cumulative_goal_rate_plot.png" alt="">
<em>Figure 6.1.3 Cumulative proportion of goals as a function of the shot probability model percentile for neural network model.</em>

<img src="/pngs/2-6-1d_best_nn_calibration_curve.png" alt=""> 
<em>Figure 6.1.4 Calibration curve for neural network model.</em>

Considering the trade-offs between different metrics of interest, we believe that our "best" performing model was the Random Forest with grid search (Figures 6.1.5-8). It achieved one of the highest F1 scores of the models and techniques tested, but also achieved a better ability to discern "quality" shots as measured by the emphasis it places on the top 10 percent of shots in its model (Figure 6.1.6) and the steeper slope of the cumulative goal rate plot (Figure 6.1.7). The calibration curve for this model shows that the model is rather well calibrated for most predicted probabilities.

<img src="/pngs/2-6-1a_rf_grid_search_best_model_params_roc_curve.png" alt="">
<em>Figure 6.1.5 ROC curve for Random Forest model with random grid search and cross-validation.</em>

<img src="/pngs/2-6-1b_rf_grid_search_best_model_params_goal_rate_plot.png" alt="">
<em>Figure 6.1.6 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for Random Forest model with random grid search and cross-validation.</em>

<img src="/pngs/2-6-1c_rf_grid_search_best_model_params_cumulative_goal_rate_plot.png" alt="">
<em>Figure 6.1.7 Cumulative proportion of goals as a function of the shot probability model percentile for Random Forest model with random grid search and cross-validation.</em>

<img src="/pngs/2-6-1d_rf_grid_search_best_model_params_calibration_curve.png" alt=""> 
<em>Figure 6.1.8 Calibration curve for Random Forest model with random grid search and cross-validation.</em>

The models have been registered at https://www.comet.com/ift6758-project#model-registry with the following names: best-nn, best-svm-prob, df-grid-search-best-model-params, gaussian-naive-bayes, rf-grid-search-best-model-params.

# 7. Evaluate on test set

## Question 7.1

Presented below (Figures 7.1.1-4) are the results of applying the previously-discussed models to the 2019/20 regular season dataset. As could be expected, for most of our models there was a slight decrease in performance on the test set compared to our validation experiments. Most models decreased by approximately one unit on each metric - for example, XGBoost had an ROC AUC of 0.81 on the validation set and 0.80 on the test with regular season set. This is well within the bounds of experimental variability, suggesting that our models largely generalize well between 2015-2018 regular seasons and the 2019/20 regular season. In general, tree-based methods (XGBoost, Random Forest and Decision Trees) performed the best and more linear discriminant methods such as SVM and logistic regression performed the worst. Surprisingly, we observed a slight increase in ROC AUC (increase of 0.02) on logistic regression models trained using distance or distance vs angle. More tests would be needed to check whether the distribution of distances from the net has changed between the 2019/20 season and previous seasons; if so, it would indicate that distance is becoming more predictive of shot success over time.

<img src="/pngs/2-7-1a_ROC_curves.png" alt="">
<em>Figure 7.1.1 ROC curves for models developed in this milestone, tested on the 2019/20 regular season dataset. Abbreviations are: LR_D - logistic regression trained on distance alone ; LR_A - logistic regression trained on angle alone ; LR_DA - logistic regression trained on distance and angle; XGB - XGBoost; RF - Random Forest; DT - Decision Tree; NN - Neural Network; SVM - Support Vector Machine.</em>

<img src="/pngs/2-7-1b_goal_rates.png" alt="">
<em>Figure 7.1.2 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for models developed in this milestone, tested on the 2019/20 regular season dataset. Abbreviations are: LR_D - logistic regression trained on distance alone ; LR_A - logistic regression trained on angle alone ; LR_DA - logistic regression trained on distance and angle; XGB - XGBoost; RF - Random Forest; DT - Decision Tree; NN - Neural Network; SVM - Support Vector Machine.</em>

<img src="/pngs/2-7-1c_goal_proportions.png" alt="">
<em>Figure 7.1.3 Cumulative proportion of goals as a function of the shot probability model percentile for models developed in this milestone, tested on the 2019/20 regular season dataset. Abbreviations are: LR_D - logistic regression trained on distance alone ; LR_A - logistic regression trained on angle alone ; LR_DA - logistic regression trained on distance and angle; XGB - XGBoost; RF - Random Forest; DT - Decision Tree; NN - Neural Network; SVM - Support Vector Machine.</em>

<img src="/pngs/2-7-1d_calibration_plots.png" alt=""> 
<em>Figure 7.1.4 Calibration curve for models developed in this milestone, tested on the 2019/20 regular season dataset. Abbreviations are: LR_D - logistic regression trained on distance alone ; LR_A - logistic regression trained on angle alone ; LR_DA - logistic regression trained on distance and angle; XGB - XGBoost; RF - Random Forest; DT - Decision Tree; NN - Neural Network; SVM - Support Vector Machine.</em>

We saw the largest decrease in performance when looking at the goal rate curves - most of our models put 5-8% less weight on goals in the upper 10th model percentile when using the regular season dataset, even though there was no corresponding change in the general shape of the cumulative percentage of goals as a function of shot probability model percentile. One hypothesis to explain why cumulative percentage is not changing while goal rate is changing is that a particular type of shot is favoured by the model, and that this favoured shot type is found in different abundance in the 2019/20 regular season dataset compared to previous years. Again, further follow-up would be required to investigate this possibility.

Most of our models were less well calibrated on the test set compared to the training set. This is not unexpected, since the model would be generally less confident of predictions on a new dataset than on the training set. Interestingly, the neural network model showed the least perturbation of calibration between the train and test sets, suggesting that it is perhaps more robust to changes in the underlying data.

## Question 7.2

When we applied our models to the 2019/20 playoff games, the curves that we obtained were noticeably spikier (Figures 7.2.1-4) than other curves we had generated, presumably because of the smaller sample size associated with using a dataset of only playoffs for only one season. It is difficult to make confident conclusions on these data, as is demonstrated by the high variability and poor calibration we see for models using this dataset (Figure 7.2.4). The tree-based methods seem particularly sensitive to this, with XGBoost, Random Forest and Decision Tree models having virtually no correspondence to the calibration line past a mean predicted probability of 0.5.

We generally saw the same ROC curves and AUC values as on the regular season games (with a slightly lower performance than with 2019/20 regular season), with the notable exception of logistic regression classifiers trained on distance (distance alone, or distance + angle) whose performance suffered greatly on this dataset. We saw the same trend when it came to the cumulative percent of goals as a function of shot probability model percentile. This suggests that there might be something fundamentally different about playoff games when it comes to distance being predictive of scoring. The other models are presumably less affected by this difference because they use many other features and so the distance signal does not dominate as strongly.

It is interesting to consider the nature of the playoff game dataset when thinking about these differences. Not only are there far fewer playoff games than regular season games, but these games are played only with a subset of teams. These teams may be "better" than the aggregate, and may use different strategies (e.g. different types of shot or distances from the net) compared to the aggregate of all teams that we see during the regular season. Stratifying by team would probably help to disentangle this and to help us see whether there are certain sub-strategies that are more successful than average.

<img src="/pngs/2-7-2a_ROC_curves.png" alt="">
<em>Figure 7.2.1 ROC curves for models developed in this milestone, tested on 2019/20 playoff games. Abbreviations are: LR_D - logistic regression trained on distance alone ; LR_A - logistic regression trained on angle alone ; LR_DA - logistic regression trained on distance and angle; XGB - XGBoost; RF - Random Forest; DT - Decision Tree; NN - Neural Network; SVM - Support Vector Machine.</em>

<img src="/pngs/2-7-2b_goal_rates.png" alt="">
<em>Figure 7.2.2 Goal rate (# goals / (#no_goals + # goals)) as a function of shot probability model percentile for models developed in this milestone, tested on 2019/20 playoff games. Abbreviations are: LR_D - logistic regression trained on distance alone ; LR_A - logistic regression trained on angle alone ; LR_DA - logistic regression trained on distance and angle; XGB - XGBoost; RF - Random Forest; DT - Decision Tree; NN - Neural Network; SVM - Support Vector Machine.</em>

<img src="/pngs/2-7-2c_goal_proportions.png" alt="">
<em>Figure 7.2.3 Cumulative proportion of goals as a function of the shot probability model percentile for models developed in this milestone, tested on 2019/20 playoff games. Abbreviations are: LR_D - logistic regression trained on distance alone ; LR_A - logistic regression trained on angle alone ; LR_DA - logistic regression trained on distance and angle; XGB - XGBoost; RF - Random Forest; DT - Decision Tree; NN - Neural Network; SVM - Support Vector Machine.</em>

<img src="/pngs/2-7-2d_calibration_plots.png" alt=""> 
<em>Figure 7.2.4 Calibration curve for models developed in this milestone, tested on ÃŸ2019/20 playoff games. Abbreviations are: LR_D - logistic regression trained on distance alone ; LR_A - logistic regression trained on angle alone ; LR_DA - logistic regression trained on distance and angle; XGB - XGBoost; RF - Random Forest; DT - Decision Tree; NN - Neural Network; SVM - Support Vector Machine.</em>
